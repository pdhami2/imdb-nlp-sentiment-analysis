{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: Sentiment Analysis for ImDb Movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Dataset\n",
    "\n",
    "\n",
    "> IMDB dataset having 50K movie reviews for natural language processing or Text analytics.\n",
    "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training and 25,000 for testing. So, predict the number of positive and negative reviews using either classification or deep learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Link to the Dataset](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Libraries and Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1 Import the libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "import seaborn as sns\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 Download Modules from nltk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/priyankadhamija/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/priyankadhamija/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/priyankadhamija/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/priyankadhamija/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(['stopwords', 'movie_reviews', 'wordnet'])\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3 Read the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Imdb Dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-49f75d564f43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display.max_colwidth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Dispay the entire row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'Imdb Dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Imdb Dataset.csv'"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None) #Dispay the entire row\n",
    "df = pd.read_csv(r'Imdb Dataset.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Manipulation & EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of the dataset:', df.shape)\n",
    "print('Summary of the dataset:') \n",
    "df.describe() #There are a few duplicates in the reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 View and Drop duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the duplicates\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1.1 Verify if there are any null values, No nulls**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.isnull().any(axis = 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1.2 Remove the numeric digits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = [re.sub(r'\\b(?:\\d+|\\w)\\b\\s*', '', x) for x in df['review']]\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Check class balance. Class looks balanced**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 Plot sentiment class distribution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3.1 Plot Pie Chart for % Class Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_sort = df['sentiment'].value_counts().sort_values(ascending = False)\n",
    "sentiment_sort.plot.pie(autopct=\"%1.1f%%\", colors = [\"wheat\", \"lavender\"], labels = sentiment_sort.index, startangle=45 )\n",
    "plt.title(\"Distribution of Sentiments %\")\n",
    "plt.axes().set_ylabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3.2 Plot Bar Chart for Abs Class Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(df.sentiment, palette =  [\"wheat\", \"lavender\"])\n",
    "plt.title('Distribution of Sentiments')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4 Remove html tags and convert to lower case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to remove html tags\n",
    "def remove_html_tags(col):\n",
    "    col = (col.str.replace(r'<[^<>]*>', '', regex=True))\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].astype(str).str.lower()\n",
    "df['review'] = remove_html_tags(df['review'])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop the duplicates again**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "#df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Use NLTK to analyze the Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 Lemmatize the words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(text):\n",
    "    words = text.split()\n",
    "    words = [WordNetLemmatizer().lemmatize(word,pos='v') for word in words]\n",
    "    return ' '.join(words)\n",
    "df['review'] = df['review'].apply(lemmatize_words)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 Remove English stopwords from nltk**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2.1 Load English stopwords from nltk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2.2 Add other stopwords like movie**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.extend(['movies', 'movie', 'film', 'films'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2.3 Tokenize the words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an additional column with tokenized words\n",
    "regexp = RegexpTokenizer('\\w+')\n",
    "df['review_token']=df['review'].apply(regexp.tokenize)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2.4 Remove the Stopwords from token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_token'] = df['review_token'].apply(lambda x: [item for item in x if item not in stopwords])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3 Create a list of all words to check the frequency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join all words to create a list\n",
    "df['review_cleaned'] = df['review_token'].apply(lambda x: ' '.join([item for item in x if len(item)>1]))\n",
    "\n",
    "all_words = ' '.join([word for word in df['review_cleaned']])\n",
    "\n",
    "# Tokenize all words together\n",
    "tokenized_words = nltk.tokenize.word_tokenize(all_words)\n",
    "fdist = FreqDist(tokenized_words)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4 Check the distribution for word frequency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_frequency = pd.DataFrame(list(fdist.items()), columns = [\"Word\",\"Frequency\"]).sort_values(by = 'Frequency', ascending = False).reset_index(drop = True)\n",
    "df_word_frequency.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count total words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_frequency.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4.1 Check how many words occur only once and the distribution of words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_word_frequency.describe())\n",
    "print('Number of words that occur only once:', (df_word_frequency[df_word_frequency['Frequency']==1].count()) )\n",
    "#Power distribution Law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight 1: 40% of the total words in all reviews occur only once**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4.2 Plot the Frequency Distribution of Top Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freq = df_word_frequency[ df_word_frequency['Frequency'] > 1]\n",
    "fig = plt.figure(figsize = (15, 5))\n",
    "plt.bar(df_freq['Word'].head(40), df_freq['Frequency'].head(40), color = 'blue', width = 0.8)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight 2: Words like movie and film have the largest occurences.**\n",
    "We removed those words because they don't add much value to predicting the sentiment in the movie dataset. After removing them words like one, make, like have the highest occurences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4.3 Show all words like movie**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[col for col in df_word_frequency['Word'] if 'movie' in col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight 3: There are typing errors like forgetting to add space between two words. \n",
    "This might be one of the reasons why 40% of the words have only single occurence**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4.4 Look at the words with least occurence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_frequency.describe() # Power Law Distribution: Natural Law Language Statistics\n",
    "df_word_frequency.tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5 Show WordCloud for Positive and Negative Reviews**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5.1 Build the function for wordcloud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud_text(df, col_name):\n",
    "    review_words = ' '.join([word for word in df[col_name]])\n",
    "    return review_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5.2 WordCloud for Positive Reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = wordcloud_text(df[df.sentiment == 'positive'], 'review_cleaned')\n",
    "plt.figure(figsize=(10,10))\n",
    "positive_wordcloud = WordCloud(max_words=400, width=3000, height=1500, background_color = 'white').generate(positive_words)\n",
    "plt.imshow(positive_wordcloud, interpolation='bilinear')\n",
    "plt.title('Frequent words in Positive Reviews', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5.3 WordCloud for Positive Reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_words = wordcloud_text(df[df.sentiment == 'negative'], 'review_cleaned')\n",
    "plt.figure(figsize=(10,10))\n",
    "negative_wordcloud = WordCloud(max_words=400, width=3000, height=1500, background_color = 'white').generate(negative_words)\n",
    "plt.imshow(negative_wordcloud, interpolation='bilinear')\n",
    "plt.title('Frequent words in Negative Reviews', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Split data into train and test (70/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, test = train_test_split(df, test_size=0.30, random_state=10)\n",
    "# Define independent variable (x) and dependent variable (y)\n",
    "train_x, train_y = training.review.tolist(), training.sentiment.tolist()\n",
    "# train_x, train_y = [training.review for x in training], [training.sentiment for y in training]\n",
    "\n",
    "# Test Dataset\n",
    "test_x, test_y = test.review.tolist(), test.sentiment.tolist()\n",
    "\n",
    "\n",
    "print(\"Size of train dataset: \",training.shape )\n",
    "print(\"Size of test dataset: \",test.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the class balance for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training['sentiment'].value_counts(normalize = 'True').round(3))\n",
    "print(test['sentiment'].value_counts(normalize = 'True').round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bag of Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using countvectorizer in sklearn, create a bag of words\n",
    "corpus = df['review_cleaned'].to_list()\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "#print(vectorizer.get_feature_names()) # the dictionary\n",
    "#print(vectorizer.fit_transform(corpus).toarray())\n",
    "\n",
    "\n",
    "# Convert train data to feature vector matrix\n",
    "train_x_vectors = vectorizer.fit_transform(train_x)\n",
    "print('Shape of Train Data Vector', train_x_vectors.shape)\n",
    "\n",
    "# Convert test data to feature vector matrix\n",
    "test_x_vectors = vectorizer.transform(test_x)\n",
    "print('Shape of Test Data Vector', test_x_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec \n",
    "Word2Vec is a method for generating word embeddings. It uses neural network to learn from a large corpus of documents. Word2Vec has advantages over one hot encoding as Word2Vec captures the semantic relationship between words. It's a transformation method. Once we generate the word embedding, we can use models like Naive Bayes to predict the accuracy. Another model that can be used for embeddings is Bert.\n",
    "\n",
    "### Bert\n",
    "Bert is a bidirectional model i.e instead of reading the text from left to right or right to left it reads the text from both sides. It's a fairly new model launched by Google in 2018. It has been trained on large corpus of data. It's a transformer based language model.\n",
    "\n",
    "### tf-idf\n",
    "tf-idf measures the importance of a word to the document. It adjusts for the Power Law of Distribution in linguistics and assigns the weighting factor accordingly. \n",
    "\n",
    "Term frequency is the relative frequency of term within the document. The inverse document frequency measures how much information does the word provide. It is the relative frequency within the set of documents. Weights generally tend to filter out the common words.\n",
    "\n",
    "tf-idf = tf*idf </br>\n",
    "tf = # of times terms appears in document/ # of terms in the document\n",
    "idf = log (# of documents in the corpus/ # of documents in the corpus containing the term)\n",
    "\n",
    "\n",
    "**For this assignment, we're going to try out the Word2Vec, tf-idf and One Hot Encoding Techniques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to convert Words to Embeddings using W2V\n",
    "\n",
    "model = Word2Vec(train_x, vector_size=500, window=5, min_count=1)\n",
    "def w2v_embedding(col):\n",
    "    return_list = []\n",
    "    for document in col:\n",
    "        embeddings = [model.wv[word] for word in document if word in model.wv]\n",
    "        average_embedding = sum(embeddings) / len(embeddings)\n",
    "        return_list.append(average_embedding)\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to word embeddings using word2vec\n",
    "train_x_w2v = w2v_embedding(train_x)\n",
    "test_x_w2v = w2v_embedding(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_x_w2v))\n",
    "print(len(test_x_w2v))\n",
    "#train_x_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train Different Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 SVM Classifier using Word2Vec\n",
    "Support Vector Machine builds a hyperplane or set of hyper planes in a highly dimesional place. It uses functinal margin (largest distance to nearest training data point) to build the hyperplane. \n",
    "\n",
    "SVM is a good model for high dimensional datasets but doesn't work well on very large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SVM classifier\n",
    "clf_svm = svm.SVC(kernel='linear')\n",
    "clf_svm.fit(train_x_w2v, train_y)\n",
    "\n",
    "# Evaluate the model on test set\n",
    "pred_svm = clf_svm.predict(test_x_w2v)\n",
    "accuracy = accuracy_score(test_y, pred_svm)\n",
    "\n",
    "print(f\"Test set accuracy: {accuracy:.3f}\")\n",
    "print(classification_report(test_y, pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Logistic Regression using One Hot Encoding\n",
    "Logistic Regression is a classic classification model. It takes the output of a logistic regression function and uses a sigmoid function to estimate te probability of the given class. <br> <br>\n",
    "There are three types of Logistic Regressions:\n",
    "<br> 1) **Binomial**: Only 2 possible DV (0/1, True/False, Pass/Fail)\n",
    "<br> 2) **Multinomial**: 3 or more possible unodered DV (cat/dog/cow)\n",
    "<br> 3) **Ordinal**: 3 or more possible odered DV (low/medium/high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Logistic Regression model\n",
    "clf_lr = LogisticRegression(solver='liblinear')\n",
    "clf_lr.fit(train_x_vectors, train_y)\n",
    "\n",
    "# Evaluate the model on test set\n",
    "pred_lr = clf_lr.predict(test_x_vectors)\n",
    "accuracy = accuracy_score(test_y, pred_lr)\n",
    "\n",
    "print(f\"Test set accuracy: {accuracy:.3f}\")\n",
    "print(classification_report(test_y, pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Random Forest Classifier Using One Hot Encoding\n",
    "It constructs multiple decision trees and can handle both classification and regression. RF searches for the best feature among a subset of random features. It adds randomness to the model thus preventing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Random Forest model\n",
    "clf_rfc = RandomForestClassifier()\n",
    "clf_rfc.fit(train_x_vectors, train_y)\n",
    "\n",
    "# Evaluate the model on test set\n",
    "pred_rfc = clf_rfc.predict(test_x_vectors)\n",
    "accuracy = accuracy_score(test_y, pred_rfc)\n",
    "\n",
    "print(f\"Test set accuracy: {accuracy:.3f}\")\n",
    "print(classification_report(test_y, pred_rfc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4  Naive Bayes using Term Frequency Inverse Document Frequency (tf-idf) embeddings\n",
    "tf-idf measures the importance of a word to the document. It adjusts for the Power Law of Distribution in linguistics and assigns the weighting factor accordingly. \n",
    "\n",
    "Term frequency is the relative frequency of term within the document. The inverse document frequency measures how much information does the word provide. It is the relative frequency within the set of documents. Weights generally tend to filter out the common words.\n",
    "\n",
    "tf-idf = tf*idf </br>\n",
    "tf = # of times terms appears in document/ # of terms in the document\n",
    "idf = log (# of documents in the corpus/ # of documents in the corpus containing the term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the TFIDF model\n",
    "clf_tdfidf = TfidfVectorizer()\n",
    "clf_tdfidf.fit(train_x, train_y)\n",
    "\n",
    "# Apply tf-idf to training data\n",
    "train_x_tf = clf_tdfidf.fit_transform(train_x)\n",
    "#applying tf idf to training data\n",
    "test_x_tf = clf_tdfidf.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Naive Bayes Classifier\n",
    "clf_nb = MultinomialNB()\n",
    "clf_nb.fit(train_x_tf, train_y)\n",
    "\n",
    "# Evaluate the model on test set\n",
    "pred_nb = clf_nb.predict(test_x_tf)\n",
    "accuracy = accuracy_score(test_y, pred_nb)\n",
    "\n",
    "print(f\"Test set accuracy: {accuracy:.3f}\")\n",
    "print(classification_report(test_y, pred_nb))\n",
    "                                                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of all the models tested, model X has the highest accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps:\n",
    "1) Fine Tuning Some of these models <br>\n",
    "2) Post-hoc analysis on where the model did and didn't do well <br>\n",
    "3) Re-applying those learning to fine tune and clean further to improve the accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
